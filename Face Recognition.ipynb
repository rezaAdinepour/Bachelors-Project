{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaAdinepour/Bachelors-Project/blob/main/Face%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "6d930ad2-e77a-4193-8bc8-81f0ca3f0707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.py\n",
            "__pycache__\n",
            "haarcascade_eye.xml\n",
            "haarcascade_eye_tree_eyeglasses.xml\n",
            "haarcascade_frontalcatface.xml\n",
            "haarcascade_frontalcatface_extended.xml\n",
            "haarcascade_frontalface_alt.xml\n",
            "haarcascade_frontalface_alt2.xml\n",
            "haarcascade_frontalface_alt_tree.xml\n",
            "haarcascade_frontalface_default.xml\n",
            "haarcascade_fullbody.xml\n",
            "haarcascade_lefteye_2splits.xml\n",
            "haarcascade_licence_plate_rus_16stages.xml\n",
            "haarcascade_license_plate_rus_16stages.xml\n",
            "haarcascade_lowerbody.xml\n",
            "haarcascade_profileface.xml\n",
            "haarcascade_righteye_2splits.xml\n",
            "haarcascade_russian_plate_number.xml\n",
            "haarcascade_smile.xml\n",
            "haarcascade_upperbody.xml\n"
          ]
        }
      ],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
        "cv2.CascadeClassifier( path )\n",
        "\n",
        "filenames = os.listdir(cv2.data.haarcascades)\n",
        "filenames = sorted(filenames)\n",
        "print('\\n'.join(filenames))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# based on: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n",
        "#\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "\n",
        "def init_camera():\n",
        "  \"\"\"Create objects and functions in HTML/JavaScript to access local web camera\"\"\"\n",
        "\n",
        "  js = Javascript('''\n",
        "\n",
        "    // global variables to use in both functions\n",
        "    var div = null;\n",
        "    var video = null;   // <video> to display stream from local webcam\n",
        "    var stream = null;  // stream from local webcam\n",
        "    var canvas = null;  // <canvas> for single frame from <video> and convert frame to JPG\n",
        "    var img = null;     // <img> to display JPG after processing with `cv2`\n",
        "\n",
        "    async function initCamera() {\n",
        "      // place for video (and eventually buttons)\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      // <video> to display video\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      div.appendChild(video);\n",
        "\n",
        "      // get webcam stream and assing to <video>\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      // start playing stream from webcam in <video>\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // <canvas> for frame from <video>\n",
        "      canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      //div.appendChild(input_canvas); // there is no need to display to get image (but you can display it for test)\n",
        "\n",
        "      // <img> for image after processing with `cv2`\n",
        "      img = document.createElement('img');\n",
        "      img.width = video.videoWidth;\n",
        "      img.height = video.videoHeight;\n",
        "      div.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function takeImage(quality) {\n",
        "      // draw frame from <video> on <canvas>\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      // stop webcam stream\n",
        "      //stream.getVideoTracks()[0].stop();\n",
        "\n",
        "      // get data from <canvas> as JPG image decoded base64 and with header \"data:image/jpg;base64,\"\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "      //return canvas.toDataURL('image/png', quality);\n",
        "    }\n",
        "\n",
        "    async function showImage(image) {\n",
        "      // it needs string \"data:image/jpg;base64,JPG-DATA-ENCODED-BASE64\"\n",
        "      // it will replace previous image in `<img src=\"\">`\n",
        "      img.src = image;\n",
        "      // TODO: create <img> if doesn't exists, \n",
        "      // TODO: use `id` to use different `<img>` for different image - like `name` in `cv2.imshow(name, image)`\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "\n",
        "  display(js)\n",
        "  eval_js('initCamera()')\n",
        "\n",
        "def take_frame(quality=0.8):\n",
        "  \"\"\"Get frame from web camera\"\"\"\n",
        "\n",
        "  data = eval_js('takeImage({})'.format(quality))  # run JavaScript code to get image (JPG as string base64) from <canvas>\n",
        "\n",
        "  header, data = data.split(',')  # split header (\"data:image/jpg;base64,\") and base64 data (JPG)\n",
        "  data = b64decode(data)  # decode base64\n",
        "  data = np.frombuffer(data, dtype=np.uint8)  # create numpy array with JPG data\n",
        "\n",
        "  img = cv2.imdecode(data, cv2.IMREAD_UNCHANGED)  # uncompress JPG data to array of pixels\n",
        "\n",
        "  return img\n",
        "\n",
        "def show_frame(img, quality=0.8):\n",
        "  \"\"\"Put frame as <img src=\"data:image/jpg;base64,....\"> \"\"\"\n",
        "\n",
        "  ret, data = cv2.imencode('.jpg', img)  # compress array of pixels to JPG data\n",
        "\n",
        "  data = b64encode(data)  # encode base64\n",
        "  data = data.decode()  # convert bytes to string\n",
        "  data = 'data:image/jpg;base64,' + data  # join header (\"data:image/jpg;base64,\") and base64 data (JPG)\n",
        "\n",
        "  eval_js('showImage(\"{}\")'.format(data))  # run JavaScript code to put image (JPG as string base64) in <img>\n",
        "                                           # argument in `showImage` needs `\" \"` "
      ],
      "metadata": {
        "id": "mfbKo6hPKzRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# based on: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=zo9YYDL4SYZr\n",
        "#\n",
        "\n",
        "#from google.colab.patches import cv2_imshow  # I don't use it but own function `show_frame()`\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "PATH = '/content/drive/MyDrive/FD/Source/dataset'\n",
        "face_detector = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "# init JavaScript code\n",
        "init_camera()\n",
        "face_id = input('\\n Enter user ID end press <Enter> ==>  ')\n",
        "print(\"\\n Initializing face capture. Look the camera and wait ...\")\n",
        "count = 0\n",
        "while True:\n",
        "    try:\n",
        "        img = take_frame()\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        #cv2_imshow(gray)  # it creates new image for every frame (it doesn't replace previous image) so it is useless\n",
        "        #show_frame(gray)  # it replace previous image\n",
        "\n",
        "        faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "        for (x, y, w, h) in faces:\n",
        "          cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
        "          count += 1\n",
        "          # Save the captured image into the datasets folder\n",
        "          cv2.imwrite(PATH + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h, x:x+w])\n",
        "          cv2.imshow('image', img)\n",
        "\n",
        "        k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
        "        if (k == 27):\n",
        "          break\n",
        "        elif count >= 30: # Take 30 face sample and stop video\n",
        "          break\n",
        "    except Exception as err:\n",
        "        print('Exception:', err)\n",
        "print(\"\\n [INFO] Exiting Program and cleanup stuff\")"
      ],
      "metadata": {
        "id": "pmFNkF0UK17L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**02-face_training phase**"
      ],
      "metadata": {
        "id": "ncqQSdMWIaUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH = '/content/gdrive/MyDrive/FD/Source/dataset'\n",
        "\n",
        "os.chdir(r'/content/gdrive/MyDrive/FD/Source/haarcascades')\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "detector = cv2.CascadeClassifier(r'/content/gdrive/MyDrive/FD/Source/haarcascades/haarcascade_frontalface_default.xml')\n",
        "\n",
        "def getImagesAndLabels(path):\n",
        "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
        "    faceSamples=[]\n",
        "    ids = []\n",
        "    for imagePath in imagePaths:\n",
        "        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
        "        img_numpy = np.array(PIL_img,'uint8')\n",
        "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
        "        faces = detector.detectMultiScale(img_numpy)\n",
        "        for (x,y,w,h) in faces:\n",
        "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
        "            ids.append(id)\n",
        "    return faceSamples,ids\n",
        "print (\"\\n [INFO] Training faces. It will take a few seconds. Wait ...\")\n",
        "\n",
        "faces,ids = getImagesAndLabels(PATH)\n",
        "recognizer.train(faces, np.array(ids))\n",
        "# Save the model into trainer/trainer.yml\n",
        "recognizer.write(r'/content/gdrive/MyDrive/FD/Source/trainer/trainer.yml') # recognizer.save() worked on Mac, but not on Pi\n",
        "# Print the numer of faces trained and end program\n",
        "print(\"\\n [INFO] {0} faces trained. Exiting Program\".format(len(np.unique(ids))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCsC4xlmEOyB",
        "outputId": "feb35ee2-2840-489a-980a-4f9abdb04888"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            " [INFO] Training faces. It will take a few seconds. Wait ...\n",
            "\n",
            " [INFO] 2 faces trained. Exiting Program\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}